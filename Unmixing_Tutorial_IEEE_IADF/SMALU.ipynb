{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONPecBYdBTaQ3RCpQoiWbk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BehnoodRasti/Unmixing_Tutorial_IEEE_IADF/blob/main/SMALU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse (Semisupervised) Unmixing - SMALU\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "original implementation found at https://github.com/YuanhangLin/Whispers2022Unmixing\n",
        "\n",
        "Published in\n",
        "\n",
        "Y. Lin and P Gader, \"ADDRESSING SPECTRAL VARIABILITY IN HYPERSPECTRAL UNMIXING WITH UNSUPERVISED NEURAL NETWORKS,\" in IEEE WHISPERS.\n",
        "\n",
        "## Imports\n",
        "\n",
        "1. Clone required github repositories\n",
        "2. Import Python packages"
      ],
      "metadata": {
        "id": "-__jQKPKqDCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/BehnoodRasti/SUnCNN.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_GQVV0czZk6",
        "outputId": "603989a2-2e38-4ecd-ab2e-38c35499577f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'SUnCNN' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yuF-_RNpnRqa"
      },
      "outputs": [],
      "source": [
        "from scipy.io import loadmat\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import normalize\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pdb\n",
        "import numpy as np\n",
        "import copy\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "fname2  = \"SUnCNN/Data/DC2/Y_clean.mat\"\n",
        "mat2 = scipy.io.loadmat(fname2)\n",
        "img_np_gt = mat2[\"Y_clean\"]\n",
        "img_np_gt = img_np_gt.transpose(2,0,1)\n",
        "[p1, nr1, nc1] = img_np_gt.shape\n",
        "img_resh=np.reshape(img_np_gt,(p1,nr1*nc1))\n",
        "#%%\n",
        "fname3  = \"SUnCNN/Data/DC2/XT.mat\"\n",
        "mat3 = scipy.io.loadmat(fname3)\n",
        "A_true_np = mat3[\"XT\"]\n",
        "\n",
        "#%%\n",
        "fname4  = \"SUnCNN/Data/DC2/EE.mat\"\n",
        "mat4 = scipy.io.loadmat(fname4)\n",
        "EE = mat4[\"EE\"]\n",
        "#%%\n",
        "LibS=EE.shape[1]"
      ],
      "metadata": {
        "id": "iU03XI97zfm1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LibS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWDEYuFKHPm-",
        "outputId": "ed9cd9c6-07ec-4752-ad29-61d9010592ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "240"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riI7fhcxv-yz",
        "outputId": "d735734d-3e23-4ded-fb10-3c5a8a68f65a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#library = loadmat(\"/content/drive/MyDrive/ColabData/MUA-SparseUnmixing/spectral_library_urban.mat\", squeeze_me = True)\n",
        "#label_names = library[\"material_names\"].astype(str)\n",
        "#X = loadmat(\"/content/drive/MyDrive/ColabData/MUA-SparseUnmixing/Urban_R162.mat\", squeeze_me = True)\n",
        "#wl, X, H, W = np.linspace(400, 2500, 224)[X[\"SlectBands\"]], X[\"Y\"].astype(float) / X[\"maxValue\"], X[\"nRow\"], X[\"nCol\"]\n",
        "#X = X.T\n",
        "dtype = torch.cuda.FloatTensor\n",
        "X=img_resh.T\n",
        "\n",
        "#library.keys(), library[\"A\"].shape, X.shape"
      ],
      "metadata": {
        "id": "ezXnRlwavYEz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.Tensor(X).type(dtype).to(device)\n",
        "X.requires_grad = False\n",
        "\n",
        "E = torch.FloatTensor(EE.T).type(dtype)\n",
        "E = E.to(device)\n",
        "E.requires_grad = False"
      ],
      "metadata": {
        "id": "Q_d_btVbvYCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.modules.loss import _Loss\n",
        "\n",
        "def BrayCurtisSimilarity(X, X_hat):\n",
        "    return (torch.abs(X - X_hat).sum(dim = 1) / (X.sum(dim = 1) + X_hat.sum(dim = 1))).mean()\n",
        "\n",
        "\n",
        "def SID(X, X_hat):\n",
        "    p = (X / X.sum(dim = 1, keepdim = True)) + 1e-3\n",
        "    q = (X_hat / X_hat.sum(dim = 1, keepdim = True)) + 1e-3\n",
        "    return torch.sum(p * torch.log(p / q) + q * torch.log(q / p), dim = 1).mean()\n",
        "\n",
        "def COS(X, X_hat):\n",
        "    X_norm = X.norm(p = 2, dim = 1)\n",
        "    X_hat_norm = X_hat.norm(p = 2, dim = 1)\n",
        "    return 1 - (X * X_hat).sum(dim = 1) / (X_norm * X_hat_norm)\n",
        "\n",
        "def SAD(X, X_hat):\n",
        "    X_norm = X.norm(p = 2, dim = 1)\n",
        "    X_hat_norm = X_hat.norm(p = 2, dim = 1)\n",
        "    inner_product = (X * X_hat).sum(dim = 1)\n",
        "    return torch.acos(inner_product / (X_norm * X_hat_norm)).mean()\n",
        "\n",
        "def SIDSAM(X, X_hat):\n",
        "    X_norm = X.norm(p = 2, dim = 1)\n",
        "    X_hat_norm = X_hat.norm(p = 2, dim = 1)\n",
        "    inner_product = (X * X_hat).sum(dim = 1)\n",
        "    alpha = torch.acos(inner_product / (X_norm * X_hat_norm))\n",
        "\n",
        "    p = (X / X.sum(dim = 1, keepdim = True))\n",
        "    q = (X_hat / X_hat.sum(dim = 1, keepdim = True))\n",
        "    sid =  torch.sum(p * torch.log(p / q + 1) + q * torch.log(q / p + 1), dim = 1)\n",
        "\n",
        "    return (sid * torch.tan(alpha)).mean()\n",
        "\n",
        "def JMSAM(X, X_hat):\n",
        "    m_t, m_r = X.mean(dim = 1), X_hat.mean(dim = 1)\n",
        "    sigma_t, sigma_r = X.var(dim = 1), X_hat.var(dim = 1)\n",
        "    sigma_avg = (sigma_t + sigma_r) / 2\n",
        "    B = 1/8 * (m_t - m_r)**2 / sigma_avg + 1/2*torch.log(torch.abs(sigma_avg) / torch.sqrt(sigma_t * sigma_r))\n",
        "    X_norm, X_hat_norm = X.norm(p = 2, dim = 1), X_hat.norm(p = 2, dim = 1)\n",
        "    inner_product = (X * X_hat).sum(dim = 1)\n",
        "    alpha = torch.arccos(inner_product / (X_norm * X_hat_norm))\n",
        "    return (2*(1-torch.exp(-B))*torch.tan(alpha)).mean()\n",
        "\n",
        "def NS3(X, X_hat):\n",
        "    _, B = X.shape\n",
        "    l2_dist = torch.sqrt(((X - X_hat) ** 2).sum(dim = 1)/B)\n",
        "    X_norm = X.norm(p = 2, dim = 1)\n",
        "    X_hat_norm = X_hat.norm(p = 2, dim = 1)\n",
        "    inner_product = (X * X_hat).sum(dim = 1)\n",
        "    cos_alpha = inner_product / (X_norm * X_hat_norm)\n",
        "\n",
        "    lower, upper = torch.min(X, dim = 1)[0], torch.max(X, dim = 1)[0]\n",
        "    l2_dist = (l2_dist - lower) / (upper - lower)\n",
        "    return torch.sqrt(l2_dist**2 + (1 - cos_alpha)**2).mean()\n",
        "\n",
        "class Sparsemax(nn.Module):\n",
        "    \"\"\"Sparsemax function.\"\"\"\n",
        "\n",
        "    def __init__(self, dim=None):\n",
        "        \"\"\"Initialize sparsemax activation\n",
        "        \n",
        "        Args:\n",
        "            dim (int, optional): The dimension over which to apply the sparsemax function.\n",
        "        \"\"\"\n",
        "        super(Sparsemax, self).__init__()\n",
        "        self.dim = -1 if dim is None else dim\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Forward function.\n",
        "        Args:\n",
        "            input (torch.Tensor): Input tensor. First dimension should be the batch size\n",
        "        Returns:\n",
        "            torch.Tensor: [batch_size x number_of_logits] Output tensor\n",
        "        \"\"\"\n",
        "        # Sparsemax currently only handles 2-dim tensors,\n",
        "        # so we reshape to a convenient shape and reshape back after sparsemax\n",
        "        input = input.transpose(0, self.dim)\n",
        "        original_size = input.size()\n",
        "        input = input.reshape(input.size(0), -1)\n",
        "        input = input.transpose(0, 1)\n",
        "        dim = 1\n",
        "\n",
        "        number_of_logits = input.size(dim)\n",
        "\n",
        "        # Translate input by max for numerical stability\n",
        "        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n",
        "\n",
        "        # Sort input in descending order.\n",
        "        # (NOTE: Can be replaced with linear time selection method described here:\n",
        "        # http://stanford.edu/~jduchi/projects/DuchiShSiCh08.html)\n",
        "        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n",
        "        range = torch.arange(start=1, end=number_of_logits + 1, step=1, device=device, dtype=input.dtype).view(1, -1)\n",
        "        range = range.expand_as(zs)\n",
        "\n",
        "        # Determine sparsity of projection\n",
        "        bound = 1 + range * zs\n",
        "        cumulative_sum_zs = torch.cumsum(zs, dim)\n",
        "        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n",
        "        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n",
        "\n",
        "        # Compute threshold function\n",
        "        zs_sparse = is_gt * zs\n",
        "\n",
        "        # Compute taus\n",
        "        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n",
        "        taus = taus.expand_as(input)\n",
        "\n",
        "        # Sparsemax\n",
        "        self.output = torch.max(torch.zeros_like(input), input - taus)\n",
        "\n",
        "        # Reshape back to original shape\n",
        "        output = self.output\n",
        "        output = output.transpose(0, 1)\n",
        "        output = output.reshape(original_size)\n",
        "        output = output.transpose(0, self.dim)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"Backward function.\"\"\"\n",
        "        dim = 1\n",
        "\n",
        "        nonzeros = torch.ne(self.output, 0)\n",
        "        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n",
        "        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n",
        "\n",
        "        return self.grad_input\n",
        "\n",
        "class ParingLayer(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.rand(n_features, n_features))\n",
        "\n",
        "    def forward(self, H, V):\n",
        "        return H @ self.weight @ V.T\n",
        "\n",
        "\n",
        "def plot_all_endmembers(x, a, wl, E, labels, label_names):\n",
        "    if isinstance(a, torch.Tensor):\n",
        "        a = a.detach().cpu().numpy()\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        x = x.detach().cpu().numpy()\n",
        "    if isinstance(E, torch.Tensor):\n",
        "        E = E.detach().cpu().numpy()\n",
        "    indices = np.where(a != 0)[0]\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(wl, x.T, c = \"b\", label = \"real\")\n",
        "    plt.plot(wl, (a @ E).T, c = \"r\", label = \"reconstructed\")\n",
        "    plt.ylim([0, 1])\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for idx in indices:\n",
        "        plt.plot(wl, E[idx].T, alpha = a[idx] * 10, \n",
        "                 label = label_names[labels[idx]] + \"#\" + str(idx) + \" \" + str(np.round(a[idx], 2)))\n",
        "    plt.legend()\n",
        "    plt.ylim([0, 1])\n",
        "    plt.show()\n",
        "    \n",
        "def plot_all_endmembers_merges(x, a, wl, E, labels, label_names):\n",
        "    if isinstance(a, torch.Tensor):\n",
        "        a = a.detach().cpu().numpy()\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        x = x.detach().cpu().numpy()\n",
        "    if isinstance(E, torch.Tensor):\n",
        "        E = E.detach().cpu().numpy()\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.detach().cpu().numpy()\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(wl, x.T, c = \"b\", label = \"real\")\n",
        "    plt.plot(wl, (a @ E).T, c = \"r\", label = \"reconstructed\")\n",
        "    plt.ylim([0, 1])\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for c in range(6):\n",
        "        flag = (a > 0) & (labels == c)\n",
        "        if np.any(flag):\n",
        "            weighted_mean = np.average(E[flag], weights=a[flag], axis = 0)\n",
        "            plt.plot(wl, weighted_mean.T, label = label_names[c] + \" \" + str(np.round(a[flag].sum(), 2)))\n",
        "            # print(label_names[c], np.round(a[flag].sum(), 2))\n",
        "    plt.legend()\n",
        "    plt.ylim([0, 1])\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "# plot_all_endmembers(x[0], A[0], wl[bbl_flag == 1], E, labels_full, label_names)\n",
        "# plot_all_endmembers_merges(x[0], A[0], wl[bbl_flag == 1], E, labels_full, label_names)"
      ],
      "metadata": {
        "id": "s4g6SlTzvX_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyMLP(nn.Module):\n",
        "\n",
        "    class Embedding(nn.Module):\n",
        "        def __init__(self, n_features, library_size):\n",
        "            super().__init__()\n",
        "            self.weight = nn.Parameter(torch.rand(n_features, library_size))\n",
        "\n",
        "        def forward(self, H):\n",
        "            return H @ self.weight\n",
        "\n",
        "    def __init__(self, B, library_size):\n",
        "        super(MyMLP, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(B, B//2)\n",
        "        self.bn1 = nn.BatchNorm1d(B//2)\n",
        "        self.fc2 = nn.Linear(B//2, B//4)\n",
        "        self.bn2 = nn.BatchNorm1d(B//4)\n",
        "        self.fc3 = nn.Linear(B//4, B//8)\n",
        "        self.bn3 = nn.BatchNorm1d(B//8)\n",
        "\n",
        "        self.spectra_encoder = [(self.fc1, self.bn1, nn.ReLU()), \n",
        "                                (self.fc2, self.bn2, nn.ReLU()),\n",
        "                                (self.fc3, self.bn3, nn.Tanh())]\n",
        "\n",
        "        self.embeddings = nn.Linear(B//8, library_size, bias = False)\n",
        "\n",
        "        for fc in [self.fc1, self.fc2, self.fc3, self.embeddings]:\n",
        "            torch.nn.init.xavier_normal_(fc.weight)\n",
        "        \n",
        "    def encoding_spectra(self, X):\n",
        "        H = X\n",
        "        for fc, bn, activation in self.spectra_encoder:\n",
        "            H = fc(H)\n",
        "            if bn is not None: H = bn(H)\n",
        "            if activation is not None: H = activation(H)\n",
        "        return H\n",
        "\n",
        "    def forward(self, X):\n",
        "        H = self.encoding_spectra(X)\n",
        "        inner_product = self.embeddings(H)\n",
        "        A = Sparsemax(dim = 1)(inner_product)\n",
        "        return A\n",
        "\n",
        "\n",
        "from collections import Counter\n",
        "#ctr = Counter(labels_full.detach().cpu().numpy())\n",
        "#foo = [ctr[key] for key in range(6)]\n",
        "\n",
        "endmember_ranges, acc = [], 0\n",
        "#for i in range(6):\n",
        "#    endmember_ranges.append(np.arange(acc, acc + foo[i]))\n",
        "#    acc += foo[i]\n",
        "\n",
        "#def merge_full_to_6(A, endmember_ranges):\n",
        "#    A_merged = np.zeros((len(A), 6))\n",
        "#    for i in range(len(A)):\n",
        "#        row = A[i]\n",
        "#        for j in range(len(endmember_ranges)):\n",
        "#            A_merged[i, j] = np.sum(row[endmember_ranges[j]])\n",
        "#    return A_merged\n",
        "\n",
        "#def merge_6_to_3(A):\n",
        "#    A_merged = np.zeros((len(A), 3))\n",
        "#    gv_indices = np.array([0, 5])\n",
        "#    pervious_indices = np.array([1, 4])\n",
        "#    impervious_indices = np.array([2, 3])\n",
        "#    A_merged[:, 0] = np.sum(A[:, gv_indices], axis = 1)\n",
        "#    A_merged[:, 1] = np.sum(A[:, pervious_indices], axis = 1)\n",
        "#    A_merged[:, 2] = np.sum(A[:, impervious_indices], axis = 1)\n",
        "#    return A_merged\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "cleanup, verbose = True, False\n",
        "cleanup_threshold = 1e-2 \n",
        "lr, individual_maes, merged_maes = 10e-3, [], []\n",
        "\n",
        "for seed in range(1):\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    model = MyMLP(p1, len(E)).to(device)\n",
        "    start_time = time.time()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay = 1e-2)\n",
        "\n",
        "    for epoch in range(500):\n",
        "        model.train()\n",
        "        A = model(X)\n",
        "        \n",
        "        if epoch >= 200 and cleanup:\n",
        "            mask = torch.ones(A.shape).to(device) \n",
        "            mask[A <= cleanup_threshold] = 0\n",
        "            A *= mask\n",
        "            A /= A.sum(dim = 1, keepdim = True)\n",
        "\n",
        "        X_hat = A @ E\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss = nn.MSELoss(reduction = \"mean\")(X_hat, X) \n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(\"epoch %d, loss: %.7f\" % (epoch, total_loss.detach().item()))\n",
        "    print(\"\\nexecution time: %.3f, seed: %d,\" %(time.time() - start_time, seed), \"cleanup?\", cleanup)\n",
        "    print(\"arch:\", model.spectra_encoder, \"\\noptimizer:\", optimizer)\n",
        "\n",
        "A_est=A.T.detach().cpu().numpy()\n",
        "#    A_est = merge_full_to_6(A.detach().cpu().numpy(), endmember_ranges)\n",
        "#    fig, axs = plt.subplots(nrows=1, ncols=6, figsize = [20, 3])\n",
        "#    for c in range(6):\n",
        "#        axs[c].imshow((A_est[:, c].reshape((H, W)).T), aspect=\"auto\")\n",
        "#        axs[c].set_title(\"EST \" + label_names[c])\n",
        "#    plt.show()"
      ],
      "metadata": {
        "id": "_uU_GUPXvX8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import linalg as LA\n",
        "A_est=A.T.detach().cpu().numpy()\n",
        "A_true=A_true_np.astype(np.float32).reshape((EE.shape[1],nr1*nc1))\n",
        "\n",
        "SRE=10*np.log10(LA.norm(A_true,'fro')/LA.norm((A_true.astype(np.float32)- np.clip(A_est, 0, 1)),'fro'))\n",
        "print ('SRE: %f' % (SRE))"
      ],
      "metadata": {
        "id": "3zs-no4UvX5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(A_est.reshape(LibS,nr1*nc1))\n"
      ],
      "metadata": {
        "id": "2qvZyOpDf_b8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}